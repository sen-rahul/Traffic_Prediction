{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029ac997-2fdb-458c-8c6f-871d9b6843ce",
   "metadata": {},
   "source": [
    "# IMPORT FILES & LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e08e7c02-7b82-46ec-bb25-72b789cb73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch Geometric for graph neural networks\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress bar for training\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Spatial distance computations\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.neighbors import NearestNeighbors, BallTree\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "# HTTP requests\n",
    "import requests\n",
    "\n",
    "# import config file\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5c4fc-08c6-4fba-b475-7e4db81a8360",
   "metadata": {},
   "source": [
    "# STEP 1A: CREATE GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a880b-fbbf-43af-9309-a53e9e56ce18",
   "metadata": {},
   "source": [
    "## 1A.1 CREATE NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46529e2-cb59-4f1e-9517-b0f74eb499fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes(meta_df, station = 'freeway_id'):\n",
    "    \"\"\"\n",
    "    Create a list of unique nodes (stations) from the metadata DataFrame and generate mappings between node IDs and indices.\n",
    "\n",
    "    Parameters:\n",
    "    - meta_df: DataFrame containing metadata with node (station) identifiers.\n",
    "    - station: Column name in the DataFrame that contains the node identifiers (default is 'freeway_id').\n",
    "\n",
    "    Returns:\n",
    "    - nodes: List of unique node identifiers.\n",
    "    - node_index_map: Dictionary mapping node identifiers to their corresponding index.\n",
    "    - index_node_map: Dictionary mapping index to node identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the nodes list i.e. the stations\n",
    "    meta_df = meta_df.sort_values(by=station, ascending=True)\n",
    "    nodes = list(meta_df[station])\n",
    "    nodes = sorted(list(set(nodes)))       \n",
    "\n",
    "    # map nodes to index: create node index map & index node map\n",
    "    node_index_map = {node: i for i, node in enumerate(nodes)}\n",
    "    index_node_map = {i: node for i, node in enumerate(nodes)}\n",
    "\n",
    "    print (len(nodes), 'Nodes Created!')\n",
    "\n",
    "    return nodes, node_index_map, index_node_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efc1eb-4935-4830-b9be-673dc1d35e16",
   "metadata": {},
   "source": [
    "## 1A.2 CREATE EDGES & EDGE ATTRIBUTES "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff5ad3-67b6-404e-8ffc-46e9b31e72a7",
   "metadata": {},
   "source": [
    "1. Identify the Euclidean distance between the stations. \n",
    "2. Map only those that are within threshold radius miles.\n",
    "3. This makes it computationally efficient to run OSRM API only for the potential mappings.\n",
    "4. Map only those stations (edges) that have a driving distance less than threshold radius miles.\n",
    "5. There are other conditions too like lane type should match.\n",
    "6. After mapping, edge attributes are created using below logic.\n",
    "7. One attribute is the normalised percentage acutal distance between the stations (threshold radius-distance)/threshold radius.\n",
    "8. second attribute is the chance of coming in the road i.e. the route possibility. if same highway and direction then 1 else 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958314f2-876b-4158-a2e3-a006a6cd9f0d",
   "metadata": {},
   "source": [
    "### Get Actual Driving Distance (Open Street Map API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "515dcd90-a2e5-43b7-a320-526d0a348af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driving_distance_osm(start_lat, start_lon, end_lat, end_lon):\n",
    "    \"\"\"\n",
    "    Function to get the driving distance between two geographic points using OpenStreetMap (OSM) data.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_lat: Latitude of the starting point.\n",
    "    - start_lon: Longitude of the starting point.\n",
    "    - end_lat: Latitude of the destination point.\n",
    "    - end_lon: Longitude of the destination point.\n",
    "\n",
    "    Returns:\n",
    "    - Distance in miles between the start and end points.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = Config.osrm_path # Base URL for the OSRM (Open Source Routing Machine) API\n",
    "    url = f\"{base_url}{start_lon},{start_lat};{end_lon},{end_lat}\"\n",
    "    params = {\n",
    "        \"overview\": \"false\", # Minimize the amount of returned data\n",
    "        \"alternatives\": \"false\",  # Do not provide alternative routes\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params) # Send the request to the OSRM API\n",
    "    data = response.json()\n",
    "    \n",
    "    if data[\"code\"] == \"Ok\":\n",
    "        distance = data[\"routes\"][0][\"distance\"]  # Extract the driving distance in meters\n",
    "        return distance / 1609  # Convert to miles\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0db82-a389-4bcd-83b7-c21ae6ec2bd5",
   "metadata": {},
   "source": [
    "### Create Edges and Edge Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f37da30-5d31-4bdb-8f27-cb66aeecb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_and_attributes(meta_df, radius_miles=1):\n",
    "    \"\"\"\n",
    "    Function to create edges and edge attributes based on spatial proximity and route characteristics.\n",
    "\n",
    "    Parameters:\n",
    "    - meta_df: DataFrame containing metadata about the locations (e.g., stations).\n",
    "    - radius_miles: The radius within which to consider points as neighbors (default is 1 mile).\n",
    "\n",
    "    Returns:\n",
    "    - edges: A tensor containing pairs of indices that define the edges.\n",
    "    - edge_attributes: A tensor containing attributes for each edge (distance and route similarity).\n",
    "    \"\"\"\n",
    "    \n",
    "    edges = []\n",
    "    edge_attributes = []\n",
    "    \n",
    "    # Extract coordinates and convert to radians\n",
    "    coords = np.radians(meta_df[['latitude', 'longitude']].values)\n",
    "    \n",
    "    # Create a BallTree for efficient nearest neighbor queries\n",
    "    tree = BallTree(coords, metric='haversine')\n",
    "    \n",
    "    # Convert the radius from miles to radians\n",
    "    radius_radians = (radius_miles+0.25) / 3959.0\n",
    "    \n",
    "    # Query the tree for all points within the radius\n",
    "    # prints only the indices within the range of distance\n",
    "    indices, distances = tree.query_radius(coords, r=radius_radians, return_distance=True)\n",
    "    \n",
    "    \n",
    "    # Create adjacency matrix wrt distance and route\n",
    "    n = len(meta_df)\n",
    "    distance_matrix = np.zeros((n, n), dtype=float)\n",
    "    route_matrix = np.zeros((n, n), dtype=float)\n",
    "\n",
    "    # Loop over each point's neighbors and distances\n",
    "    for i, (idx, dist) in tqdm(enumerate(zip(indices, distances)), total=len(distances), desc=\"Creating Edges & Edge Attributes\", bar_format=\"{l_bar}{r_bar}\"):  \n",
    "        # 0-1 distance (towards 1 closer distance)\n",
    "        # Calculate a weighted distance score for neighbors (closer distance = higher score)\n",
    "        distance_matrix[i, idx] = np.where(dist>0,(radius_miles-(dist * 3959.0))/radius_miles,0)\n",
    "        \n",
    "        # Create a copy of the meta_df to find neighbors' attributes\n",
    "        meta_df_cpy = meta_df.copy().reset_index()\n",
    "        meta_df_cpy['distance'] = distance_matrix[i]\n",
    "\n",
    "        # Filter out neighbors with non-zero distances and retrieve relevant columns\n",
    "        neighbors = meta_df_cpy.loc[distance_matrix[i] > 0, ['freeway_id', 'freeway_direction','highway','type','distance', 'latitude', 'longitude']]\n",
    "\n",
    "        # Iterate through the neighbors to evaluate and assign edge attributes\n",
    "        for j, row in neighbors.iterrows():\n",
    "\n",
    "            # Only consider neighbors of the same lane type for edge creation\n",
    "            if meta_df.iloc[i].type != row['type']:\n",
    "                distance_matrix[i, j] = 0\n",
    "            else:\n",
    "                # Calculate the actual driving distance using OpenStreetMap (OSM) data\n",
    "                actual_distance = get_driving_distance_osm(row['latitude'], row['longitude'], meta_df.iloc[i].latitude, meta_df.iloc[i].longitude)\n",
    "                # If the actual distance exceeds the radius, ignore this neighbor\n",
    "                if actual_distance > radius_miles:\n",
    "                    distance_matrix[i, j] = 0    \n",
    "                else:\n",
    "                    # Assign the actual driving distance to the distance matrix\n",
    "                    distance_matrix[i, j] = np.round(actual_distance,2)\n",
    "                    # Check if same road, and assign route matrix accordingly\n",
    "                    route_matrix[i, j] = 1 if (meta_df.iloc[i].freeway_direction == row['freeway_direction']) and (meta_df.iloc[i].highway == row['highway']) and (meta_df.iloc[i].type == row['type']) else 0 \n",
    "                    edges.append((i, j))\n",
    "                    # Append the edge and its attributes to their respective lists\n",
    "                    edge_attributes.append((distance_matrix[i, j], route_matrix[i, j]))\n",
    "                    \n",
    "    # Remove self-loops\n",
    "    np.fill_diagonal(distance_matrix, 0.0)\n",
    "    np.fill_diagonal(route_matrix, 0.0)\n",
    "    edges = torch.LongTensor(edges).t().contiguous()\n",
    "    edge_attributes = torch.tensor(edge_attributes, dtype=torch.float32)\n",
    "    print ('Edges & Edge Attributes Created!!')\n",
    "    \n",
    "    return edges, edge_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff45f6-0458-4022-b3b7-89808337fef0",
   "metadata": {},
   "source": [
    "## 1A.3 CREATE GRAPH WITH NODE FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf8769-1db9-43b6-9b1d-acc8fd813912",
   "metadata": {},
   "source": [
    "1. 1 graph for each timestep (each timestep refers to the reference timestep which means that we are currently at that instant and we know the truth (traffic flow) only till that point).\n",
    "2. 41 nodes in each graph with same edges and edge attributes.\n",
    "3. For each timestep, both x (node features) and y (target) are pointed based on the inputs passed.\n",
    "4. The inputs independent_var tell the node features and dependent_var tells about the target. \n",
    "5. The inputs x_ts_start, x_ts_end, x_ts_step informs based on the current timestep to focus on which past timestep wrt to the current timestep.\n",
    "6. The inputs y_ts_start, y_ts_end, y_ts_step informs based on the current timestep to focus on which future timestep to predict wrt to the current timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebd56d3-1073-4311-931f-13853e55a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph for each timestamp\n",
    "def create_graphs(df, independent_var, dependent_var, edge_index, edge_attributes, x_ts_start = -7, x_ts_end = 5, x_ts_step=1, y_ts_start=1, y_ts_end=4, y_ts_step=1, data_interval_mins=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to create graphs based on timestamps from the given DataFrame. \n",
    "    The function generates graph data for each unique timestamp in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data with timestamps.\n",
    "    - independent_var: List of column names to be used as independent variables.\n",
    "    - dependent_var: List of column names to be used as dependent variables.\n",
    "    - edge_index: Edge indices for graph construction.\n",
    "    - edge_attributes: Attributes for the edges in the graph.\n",
    "    - x_ts_start: Start time step (in terms of intervals) for the independent variable window.\n",
    "    - x_ts_end: End time step (in terms of intervals) for the independent variable window.\n",
    "    - x_ts_step: Step size (in terms of intervals) for sampling within the independent variable window.\n",
    "    - y_ts_start: Start time step (in terms of intervals) for the dependent variable window.\n",
    "    - y_ts_end: End time step (in terms of intervals) for the dependent variable window.\n",
    "    - y_ts_step: Step size (in terms of intervals) for sampling within the dependent variable window.\n",
    "    - data_interval_mins: The interval (in minutes) between timestamps in the data.\n",
    "\n",
    "    Returns:\n",
    "    - graphs: A list of graph objects created for each unique timestamp.\n",
    "    - timestamp_sequences: A list of timestamps corresponding to each graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store the graphs\n",
    "    graphs = []\n",
    "    timestamp_sequences = []\n",
    "\n",
    "    # Convert 'iso_timestamp' column to numpy array of datetime64 type\n",
    "    timestamps = df['iso_timestamp'].values\n",
    "    timestamps = np.array(timestamps, dtype='datetime64')\n",
    "    \n",
    "    # Convert time steps into timedelta (in nanoseconds)\n",
    "    x_ts_step = np.int64(np.timedelta64(x_ts_step*data_interval_mins*60*10**9, 's'))\n",
    "    y_ts_step = np.int64(np.timedelta64(y_ts_step*data_interval_mins*60*10**9, 's'))\n",
    "    \n",
    "    \n",
    "    for timestamp in tqdm(sorted(df['iso_timestamp'].unique())):\n",
    "        \n",
    "        # Define graph\n",
    "        g = Data()\n",
    "        g.edge_index = edge_index\n",
    "        g.edge_attr = edge_attributes\n",
    "        \n",
    "        # Current iteration time in nanoseconds\n",
    "        row_time = np.int64(timestamp.timestamp()*10**9)\n",
    "\n",
    "        # Define range for dependent variable (y) timestamps\n",
    "        row_y_ts_start = row_time + np.int64(np.timedelta64(y_ts_start*data_interval_mins*60*10**9, 's'))\n",
    "        row_y_ts_end = row_time + np.int64(np.timedelta64(y_ts_end*data_interval_mins*60*10**9, 's'))\n",
    "        row_y_ts_range = list(range(row_y_ts_start, row_y_ts_end + y_ts_step, y_ts_step))\n",
    "        row_x_ts_range = np.array([])\n",
    "        \n",
    "        # Define range for independent variable (x) timestamps for each y_ts in row_y_ts_range\n",
    "        for y_ts in row_y_ts_range:\n",
    "            \n",
    "            temp_x_ts_start = y_ts + np.int64(np.timedelta64(x_ts_start*data_interval_mins*60*10**9, 's'))\n",
    "            temp_x_ts_end = y_ts + np.int64(np.timedelta64(x_ts_end*data_interval_mins*60*10**9, 's'))\n",
    "            temp_x_ts_range = np.array(list(range(temp_x_ts_start, temp_x_ts_end + x_ts_step, x_ts_step)))\n",
    "            \n",
    "            # Keep only timestamps that are earlier than or equal to the current timestamp\n",
    "            temp_x_ts_range = temp_x_ts_range[temp_x_ts_range<=row_time]\n",
    "            row_x_ts_range = np.unique(np.hstack([row_x_ts_range, temp_x_ts_range]))\n",
    "            \n",
    "\n",
    "        # Convert to datetime64 and find indices in the original DataFrame\n",
    "        row_x_ts_range = np.array(row_x_ts_range, dtype='datetime64[ns]')\n",
    "        row_x_ts_range = list(row_x_ts_range.astype(str))\n",
    "        row_x_ts_range = np.array(row_x_ts_range, dtype='datetime64')\n",
    "        row_x_indices = np.where(np.isin(timestamps, row_x_ts_range))[0]\n",
    "            \n",
    "        row_y_ts_range = np.array(row_y_ts_range, dtype='datetime64[ns]')\n",
    "        row_y_ts_range = list(row_y_ts_range.astype(str))\n",
    "        row_y_ts_range = np.array(row_y_ts_range, dtype='datetime64')\n",
    "        row_y_indices = np.where(np.isin(timestamps, row_y_ts_range))[0]\n",
    "\n",
    "        \n",
    "        # Create pivot table for independent variables (x)\n",
    "        pivot_df_x = df[['station','iso_timestamp'] + independent_var].copy().loc[sorted(row_x_indices)].pivot(index='station', columns='iso_timestamp')\n",
    " \n",
    "            \n",
    "        if len(pivot_df_x)>0: \n",
    "            \n",
    "            # Flatten multi-level columns and sort them (traffic features for x timeframe)\n",
    "            pivot_df_x.columns = [f'{col[1]}_{col[0]}' for col in pivot_df_x.columns]\n",
    "            sorted_columns = sorted(pivot_df_x.columns)\n",
    "            pivot_df_x = pivot_df_x[sorted_columns]\n",
    "            pivot_df_x = pivot_df_x.apply(pd.to_numeric)\n",
    "            g.x = torch.tensor(pivot_df_x.values, dtype=torch.float)\n",
    "\n",
    "            # Create pivot table for dependent variables (y)\n",
    "            pivot_df_y = df[['station','iso_timestamp'] + dependent_var].loc[sorted(row_y_indices)].pivot(index='station', columns='iso_timestamp')\n",
    "\n",
    "            if len(pivot_df_y.columns)>=len(row_y_ts_range)*len(dependent_var):\n",
    "                # Flatten multi-level columns and sort them\n",
    "                pivot_df_y.columns = [f'{col[1]}_{col[0]}' for col in pivot_df_y.columns]\n",
    "                sorted_columns = sorted(pivot_df_y.columns)\n",
    "                pivot_df_y = pivot_df_y[sorted_columns]\n",
    "                pivot_df_y = pivot_df_y.apply(pd.to_numeric)\n",
    "                g.y = torch.tensor(pivot_df_y.values, dtype=torch.float)\n",
    "                graphs.append(g)\n",
    "                \n",
    "                timestamp_sequences.append(timestamp)\n",
    "            else:\n",
    "                timestamp_sequences.append(timestamp)\n",
    "                \n",
    "\n",
    "    return graphs, timestamp_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587675b-c1ff-4c9d-9719-25e2b3025274",
   "metadata": {},
   "source": [
    "# STEP 1B: CREATE GRIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc10760-949c-482c-98fe-7fa929de15a1",
   "metadata": {},
   "source": [
    "## 1B.1 GRID MAPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7481761-657e-4f54-8841-724df9e897a7",
   "metadata": {},
   "source": [
    "1. Find the n nearest neigbours whose traffic value will be used to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb0a14b-de69-4c48-9eec-6c18ac4aa413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_mapping(meta_df, neighbours, node_index_map, lat_range=None, lon_range=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to create a grid mapping for stations based on their geographical proximity.\n",
    "    The mapping is created by first sorting stations by 'highway' and 'freeway_direction', \n",
    "    then grouping them and finding nearest neighbors based on geographical distance.\n",
    "\n",
    "    Parameters:\n",
    "    - meta_df: DataFrame containing station metadata including 'latitude' and 'longitude'.\n",
    "    - neighbours: Number of nearest neighbors to include in the grid for each station.\n",
    "    - node_index_map: Dictionary mapping station IDs to their respective indices.\n",
    "    - lat_range: Optional, latitude range to filter the stations.\n",
    "    - lon_range: Optional, longitude range to filter the stations.\n",
    "\n",
    "    Returns:\n",
    "    - grid: A sorted list where each entry represents a station and its nearest neighbors,\n",
    "            with station IDs mapped to their indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the metadata DataFrame by 'highway' and 'freeway_direction'\n",
    "    meta_df = meta_df.sort_values(by = ['highway','freeway_direction'])\n",
    "    grid = []\n",
    "    \n",
    "    # Function to map station IDs to their corresponding index using 'node_index_map'\n",
    "    def map_station_index(x):\n",
    "        return node_index_map[x]\n",
    "    \n",
    "    # Iterate over each unique combination of 'highway' and 'freeway_direction'\n",
    "    for row, current_data in meta_df[['highway','freeway_direction']].drop_duplicates().iterrows():\n",
    "        \n",
    "        # Filter the DataFrame for the current 'highway' and 'freeway_direction'\n",
    "        temp_station_metadata = meta_df.loc[(meta_df['highway']==current_data['highway']) \n",
    "                                                     & (meta_df['freeway_direction']==current_data['freeway_direction'])].copy()\n",
    "\n",
    "        # Calculate the pairwise haversine distances (geodesic distance) between stations\n",
    "        distances = haversine_distances(np.radians(temp_station_metadata[['latitude', 'longitude']].values))\n",
    "        # Fit a nearest neighbors model on the calculated distances\n",
    "        nbrs = NearestNeighbors(n_neighbors = neighbours+1, algorithm='ball_tree').fit(distances)\n",
    "        _, indices = nbrs.kneighbors(distances)\n",
    "\n",
    "        # For each station in the current group\n",
    "        for i in range(len(temp_station_metadata)):\n",
    "\n",
    "            # Find the indices of the nearest neighbors, excluding the station itself\n",
    "            idx = indices[i][indices[i]!=i]\n",
    "            i_idx =  indices[i][indices[i]==i]\n",
    "\n",
    "            # Concatenate the station and its neighbors into one row\n",
    "            grid_row = pd.concat([temp_station_metadata.iloc[i_idx,0], \n",
    "                                  temp_station_metadata.iloc[idx,0]])\n",
    "            # Map station IDs to indices using the 'map_station_index' function and add to grid\n",
    "            grid.append(np.vectorize(map_station_index)(grid_row))\n",
    "    \n",
    "    # Sort the grid based on the first element of each row (the station index)\n",
    "    grid = sorted(grid, key=lambda x: x[0])\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb261d8-1527-46a1-afe7-8c3b3d55c69f",
   "metadata": {},
   "source": [
    "## 1B.2 GRID ROW FEATURE MAPPING: MAP NEIGHBOURING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9081faad-9818-4679-a1f8-287adfdbacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_spatial_grid(df, grid_mapping):\n",
    "    \"\"\"\n",
    "    Function to transform the DataFrame into a spatial grid based on the grid mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data to be transformed into the spatial grid.\n",
    "    - grid_mapping: The grid mapping created by 'create_grid_mapping' function.\n",
    "\n",
    "    Returns:\n",
    "    - torch.tensor: A PyTorch tensor representing the spatial grid, \n",
    "                    where each row corresponds to the flattened values of stations and their neighbors.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # Iterate over each grid row in the grid mapping\n",
    "    for row in grid_mapping:\n",
    "        # Flatten the data of the stations in the grid row and append to result list\n",
    "        result.append(df.iloc[row].values.flatten())\n",
    "\n",
    "    # Convert the result to a PyTorch tensor\n",
    "    return torch.tensor(np.array(result), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a5056-6302-4680-bbc2-4b9f97ead3f2",
   "metadata": {},
   "source": [
    "## 1B.3 CREATE GRID FEATURES DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383f0e8-0063-4cd1-959d-27052a1377ac",
   "metadata": {},
   "source": [
    "1. 1 grid for each timestep (each timestep refers to the reference timestep which means that we are currently at that instant and we know the truth till that point)\n",
    "2. 41 rows in each grid, each row representa a station data for that timestep\n",
    "3. For each timestep, both x (grid features) and y (target) are pointed based on the inputs passed.\n",
    "4. The inputs independent_var tell the features of both reference station and mapped neighbour stations and dependent_var tells about the target of reference station. \n",
    "5. The inputs x_ts_start, x_ts_end, x_ts_step tells based on the current timestep to focus on which past timestep wrt to the current timestep.\n",
    "6. The inputs y_ts_start, y_ts_end, y_ts_step tells based on the current timestep to focus on which future timestep to predict wrt to the current timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad525f35-01a0-4fc3-b0c3-5155a7709283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid for each timestamp\n",
    "def create_grids(df, independent_var, dependent_var, grid_mapping, x_ts_start = -7, x_ts_end = 5, x_ts_step=1, y_ts_start=1, y_ts_end=4, y_ts_step=1, data_interval_mins=5):\n",
    "    \"\"\"\n",
    "    Function to create spatial grids for each timestamp in the DataFrame.\n",
    "    The function generates grid data for each unique timestamp, using a given grid mapping\n",
    "    and time intervals to determine the relevant data points.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data with timestamps.\n",
    "    - independent_var: List of column names to be used as independent variables.\n",
    "    - dependent_var: List of column names to be used as dependent variables.\n",
    "    - grid_mapping: A mapping that defines the spatial grid structure.\n",
    "    - x_ts_start: Start time step (in terms of intervals) for the independent variable window.\n",
    "    - x_ts_end: End time step (in terms of intervals) for the independent variable window.\n",
    "    - x_ts_step: Step size (in terms of intervals) for sampling within the independent variable window.\n",
    "    - y_ts_start: Start time step (in terms of intervals) for the dependent variable window.\n",
    "    - y_ts_end: End time step (in terms of intervals) for the dependent variable window.\n",
    "    - y_ts_step: Step size (in terms of intervals) for sampling within the dependent variable window.\n",
    "    - data_interval_mins: The interval (in minutes) between timestamps in the data.\n",
    "\n",
    "    Returns:\n",
    "    - grids: A list of grid objects created for each unique timestamp.\n",
    "    - timestamp_sequences: A list of timestamps corresponding to each grid.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store the grids for each timestamp\n",
    "    grids = []\n",
    "    timestamp_sequences = []\n",
    "\n",
    "    # Convert 'iso_timestamp' column to numpy array of datetime64 type\n",
    "    timestamps = df['iso_timestamp'].values\n",
    "    timestamps = np.array(timestamps, dtype='datetime64')\n",
    "\n",
    "    # Convert time steps into timedelta (in nanoseconds)\n",
    "    x_ts_step = np.int64(np.timedelta64(x_ts_step*data_interval_mins*60*10**9, 's'))\n",
    "    y_ts_step = np.int64(np.timedelta64(y_ts_step*data_interval_mins*60*10**9, 's'))\n",
    "    \n",
    "\n",
    "    # Iterate over each unique timestamp in the DataFrame\n",
    "    for timestamp in tqdm(sorted(df['iso_timestamp'].unique())):\n",
    "        \n",
    "        # Create an empty Data object to store grid data for the current timestamp\n",
    "        g = Data()\n",
    "        \n",
    "        # Current iteration time in nanoseconds\n",
    "        row_time = np.int64(timestamp.timestamp()*10**9)\n",
    "\n",
    "        # Define range for dependent variable (y) timestamps\n",
    "        row_y_ts_start = row_time + np.int64(np.timedelta64(y_ts_start*data_interval_mins*60*10**9, 's'))\n",
    "        row_y_ts_end = row_time + np.int64(np.timedelta64(y_ts_end*data_interval_mins*60*10**9, 's'))\n",
    "        row_y_ts_range = list(range(row_y_ts_start, row_y_ts_end + y_ts_step, y_ts_step))\n",
    "        row_x_ts_range = np.array([])\n",
    "\n",
    "        \n",
    "        # Define range for independent variable (x) timestamps for each y_ts in row_y_ts_range\n",
    "        for y_ts in row_y_ts_range:\n",
    "            \n",
    "            temp_x_ts_start = y_ts + np.int64(np.timedelta64(x_ts_start*data_interval_mins*60*10**9, 's'))\n",
    "            temp_x_ts_end = y_ts + np.int64(np.timedelta64(x_ts_end*data_interval_mins*60*10**9, 's'))\n",
    "            temp_x_ts_range = np.array(list(range(temp_x_ts_start, temp_x_ts_end + x_ts_step, x_ts_step)))\n",
    "\n",
    "            # Keep only timestamps that are earlier than or equal to the current timestamp\n",
    "            temp_x_ts_range = temp_x_ts_range[temp_x_ts_range<=row_time]\n",
    "            row_x_ts_range = np.unique(np.hstack([row_x_ts_range, temp_x_ts_range]))\n",
    "            \n",
    "\n",
    "        # Convert to datetime64 and find indices in the original DataFrame\n",
    "        row_x_ts_range = np.array(row_x_ts_range, dtype='datetime64[ns]')\n",
    "        row_x_ts_range = list(row_x_ts_range.astype(str))\n",
    "        row_x_ts_range = np.array(row_x_ts_range, dtype='datetime64')\n",
    "        row_x_indices = np.where(np.isin(timestamps, row_x_ts_range))[0]\n",
    "        \n",
    "        row_y_ts_range = np.array(row_y_ts_range, dtype='datetime64[ns]')\n",
    "        row_y_ts_range = list(row_y_ts_range.astype(str))\n",
    "        row_y_ts_range = np.array(row_y_ts_range, dtype='datetime64')\n",
    "        row_y_indices = np.where(np.isin(timestamps, row_y_ts_range))[0]\n",
    "\n",
    "        # Pivot the DataFrame for independent variables (x)\n",
    "        pivot_df_x = df[['station','iso_timestamp'] + independent_var].copy().loc[sorted(row_x_indices)].pivot(index='station', columns='iso_timestamp')\n",
    " \n",
    "            \n",
    "        if len(pivot_df_x)>0: \n",
    "            \n",
    "            # Flatten the multi-level columns and sort them (traffic features)\n",
    "            pivot_df_x.columns = [f'{col[1]}_{col[0]}' for col in pivot_df_x.columns]\n",
    "            sorted_columns = sorted(pivot_df_x.columns)\n",
    "            pivot_df_x = pivot_df_x[sorted_columns]\n",
    "            pivot_df_x = pivot_df_x.apply(pd.to_numeric)\n",
    "\n",
    "            # Convert the pivot table to a spatial grid using the grid mapping\n",
    "            g.x = to_spatial_grid(pivot_df_x, grid_mapping)\n",
    "\n",
    "            # Pivot the DataFrame for dependent variables (y)\n",
    "            pivot_df_y = df[['station','iso_timestamp'] + dependent_var].loc[sorted(row_y_indices)].pivot(index='station', columns='iso_timestamp')\n",
    "\n",
    "            # Check if the pivot table has enough columns to match the expected length\n",
    "            if len(pivot_df_y.columns)>=len(row_y_ts_range)*len(dependent_var):\n",
    "                # Flatten the multi-level columns and sort them\n",
    "                pivot_df_y.columns = [f'{col[1]}_{col[0]}' for col in pivot_df_y.columns]\n",
    "                sorted_columns = sorted(pivot_df_y.columns)\n",
    "                pivot_df_y = pivot_df_y[sorted_columns]\n",
    "                pivot_df_y = pivot_df_y.apply(pd.to_numeric)\n",
    "\n",
    "                # Convert the pivot table to a PyTorch tensor and store it as the target variable\n",
    "                g.y = torch.tensor(pivot_df_y.values, dtype=torch.float)\n",
    "                grids.append(g)\n",
    "\n",
    "                # Append the grid data and corresponding timestamp to the result lists\n",
    "                timestamp_sequences.append(timestamp)\n",
    "            else:\n",
    "                # If the y data is insufficient, still store the timestamp\n",
    "                timestamp_sequences.append(timestamp)\n",
    "                \n",
    "    # Return the list of grids and corresponding timestamps\n",
    "    return grids, timestamp_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a88e3-2538-41d8-8b6b-7184b1ccbd90",
   "metadata": {},
   "source": [
    "# STEP 2. MATCH INPUT LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b954e50-f02a-4223-8a87-5eeced532485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_input_dim(data, timestamp):\n",
    "    \"\"\"\n",
    "    Function to filter and retain only those data samples where the input dimensions match\n",
    "    the desired dimensions, ensuring consistent input size for further processing.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of data objects, each containing features (x).\n",
    "    - timestamp: List of timestamps corresponding to each data object.\n",
    "\n",
    "    Returns:\n",
    "    - data_final: List of filtered data objects with matching input dimensions.\n",
    "    - timestamp_final: List of corresponding timestamps for the filtered data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_final = []\n",
    "    timestamp_final = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "\n",
    "        start_dim = data[i].x.shape[1] # Input dimension of the current data object\n",
    "        end_dim = data[-1].x.shape[1] # Input dimension of the last data object\n",
    "\n",
    "        # If the input dimensions match, retain this data object and its timestamp\n",
    "        if start_dim == end_dim:\n",
    "            data_final.append(data[i])\n",
    "            timestamp_final.append(timestamp[i])\n",
    "\n",
    "    return data_final, timestamp_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ac087-50ee-4603-81cc-3d51d0fe2a5c",
   "metadata": {},
   "source": [
    "# STEP 3. CREATE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c2edb-ccbe-438f-8fdc-093c627246a8",
   "metadata": {},
   "source": [
    "1. 1st layer can either be CNN/GCN/GAT and can have multiple parallel layers\n",
    "2. After this, the output of the 1st layer is processed, the embeddings are concatenated\n",
    "3. Then passed to a Transformer\n",
    "4. Followed by FC layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f5d7a-d659-4778-b3bf-26e06009bebc",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "384dc6bd-62e0-40bf-930b-f8b653bc6d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualGAT_Trans(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        \"\"\"\n",
    "        Initialize the DualGAT_Trans model, which is a spatial-temporal neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        - model_params: A list of dictionaries containing parameters for each layer in the network.\n",
    "                        Each dictionary specifies the model type (e.g., CNN, GCN, GAT, Transformer) \n",
    "                        and relevant parameters for that layer.\n",
    "        \"\"\"\n",
    "        super(DualGAT_Trans, self).__init__()\n",
    "\n",
    "        # Initialize a ModuleList to hold all layers in the network\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Loop through the provided model parameters to create layers\n",
    "        for params in model_params:\n",
    "            layer_no = params.pop('layer_no') # Layer number\n",
    "            model_type = params.pop('model') # Type of model (e.g., CNN, GCN, GAT, Transformer)\n",
    "\n",
    "            if layer_no == 1: # First layer can be CNN, GCN, or GAT\n",
    "                if model_type == 'CNN':\n",
    "                    self.layers.append(nn.Conv2d(**params))\n",
    "                elif model_type == 'GCN':\n",
    "                    self.layers.append(GCNConv(**params))\n",
    "                elif model_type == 'GAT':\n",
    "                    self.layers.append(GATConv(**params))\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "            \n",
    "            else: # Subsequent layers can be Transformer or Linear\n",
    "                if model_type == 'Transformer':\n",
    "                    params['batch_first'] = True # Set batch_first to True for transformers\n",
    "                    self.transformer = nn.Transformer(**params)\n",
    "                elif model_type == 'Linear':\n",
    "                    self.linear = nn.Linear(**params)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "        # Define a max-pooling layer for use with CNNs\n",
    "        self.pool = nn.MaxPool2d(kernel_size=Config.cnn_pooling_size, stride=Config.cnn_pooling_stride)\n",
    "\n",
    "    def forward(self, data_list):\n",
    "        \"\"\"\n",
    "        Forward pass for the DualGAT_Trans model.\n",
    "\n",
    "        Parameters:\n",
    "        - data_list: A list of input data objects.\n",
    "\n",
    "        Returns:\n",
    "        - x: The output of the network after passing through all layers and the final linear transformation.\n",
    "        \"\"\"\n",
    "        \n",
    "        x_list = []\n",
    "        batch_size = data_list[0].num_graphs # Number of graphs in the batch\n",
    "\n",
    "        # Apply the first layer to each data object in data_list\n",
    "        for i, data in enumerate(data_list):\n",
    "            x = data.x if hasattr(data, 'x') else data\n",
    "            edge_index = data.edge_index if hasattr(data, 'edge_index') else None\n",
    "\n",
    "            # If the layer is a GCN or GAT, apply it to the graph input\n",
    "            if isinstance(self.layers[i], (GCNConv, GATConv)):\n",
    "                x = self.layers[i](x, edge_index)\n",
    "\n",
    "            # If the layer is a CNN, reshape and apply it to the grid input\n",
    "            elif isinstance(self.layers[i], nn.Conv2d):\n",
    "                x = torch.reshape(x, (batch_size, -1, x.shape[-1]))\n",
    "                x = x.unsqueeze(1)  # Add channel dimension\n",
    "                x = F.relu(self.layers[i](x)) # Apply CNN and ReLU activation\n",
    "\n",
    "                # Apply max-pooling layers\n",
    "                for layer in range(Config.cnn_pooling_layers):\n",
    "                    x = self.pool(x)\n",
    "            \n",
    "            x = F.relu(x) # Apply ReLU activation to the output\n",
    "            x_list.append(x) # Store the output in x_list\n",
    "\n",
    "        # Reshape the embeddings before concatenation\n",
    "        reshaped_x_list = []\n",
    "        for i, x in enumerate(x_list):\n",
    "            if isinstance(self.layers[i], (GCNConv, GATConv)):\n",
    "                n_node = data_list[i].num_nodes // batch_size # Number of nodes per grap\n",
    "                x = torch.reshape(x, (batch_size, n_node, x.shape[1])) # Reshape to (batch_size, nodes, features)\n",
    "            reshaped_x_list.append(x)\n",
    "        \n",
    "        # Concatenating all inputs into a single tensor\n",
    "        x = torch.cat(reshaped_x_list, dim=-1)\n",
    "\n",
    "        # Reshape before passing into transformer\n",
    "        if len(x.shape) == 3:\n",
    "            # No need to permute if batch_first=True \n",
    "            pass\n",
    "        elif len(x.shape) == 4:\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "            x = x.reshape(x.shape[0], x.shape[1], -1) # Flatten the last two dimensions\n",
    "\n",
    "        # Pass through transformer and linear layers\n",
    "        src, tgt = x, x # Using the same tensor for source and target in transformer\n",
    "        x = self.transformer(src, tgt)\n",
    "        x = torch.squeeze(x[:, -1, :])  # Squeeze to remove unnecessary dimensions after transformer\n",
    "        x = self.linear(x) # Apply final linear transformation\n",
    "        \n",
    "        # Reshaping the output to match the original node structure\n",
    "        total_nodes = src.shape[1] # No. of nodes\n",
    "        s = x.shape\n",
    "        # print (8, s)\n",
    "        x = torch.reshape(x, (s[0], total_nodes, data_list[0].y.shape[1])) # Reshape to (batch_size, nodes, output_dim\n",
    "        x = torch.reshape(x, (s[0] * total_nodes, data_list[0].y.shape[1])) # Flatten the batch and node dimensions\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b97278-ec80-4a2b-836c-289ef49e3cf6",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b726341-fe14-4ebe-bf40-202094a5d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Training function for the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The DualGAT_Trans model to be trained.\n",
    "    - train_loader: DataLoader for the training data.\n",
    "    - optimizer: Optimizer for gradient descent.\n",
    "    - criterion: Loss function to minimize.\n",
    "\n",
    "    Returns:\n",
    "    - total_loss: Average training loss over the epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train() # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Loop through the training batches\n",
    "    for batch in zip(*train_loader):\n",
    "        optimizer.zero_grad() # Zero the gradients\n",
    "        out = model(batch) # Forward pass\n",
    "        loss = criterion(out, batch[0].y) # Compute loss\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update weights\n",
    "        total_loss += loss.item() # Accumulate loss\n",
    "        \n",
    "    return total_loss / len(train_loader[0]) # Return the average loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54c972-6468-4b1a-807e-7f5e7e08b5f9",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75c76974-d41d-4520-b253-f7efe9dad0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_transform(data, scaler, index_tf=2):\n",
    "    \"\"\"\n",
    "    Inverse transform the output data using a scaler.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Tensor of model outputs.\n",
    "    - scaler: Scaler object used for inverse transformation.\n",
    "    - index_tf: Index of the feature to inverse transform.\n",
    "\n",
    "    Returns:\n",
    "    - data: Tensor of inverse-transformed data, with negative values set to 0 and rounded to integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_shape = data.shape\n",
    "    data = data.detach().cpu().numpy().reshape(-1) # Convert tensor to NumPy array and flatten\n",
    "    dummy_array = np.zeros((len(data), scaler.n_features_in_))  # Create a dummy array for inverse transform\n",
    "    dummy_array[:, index_tf] = data # Place the data in the correct feature column\n",
    "    \n",
    "    # perform inverse transform\n",
    "    inverse_transformed = scaler.inverse_transform(dummy_array)\n",
    "    data = inverse_transformed[:, index_tf] # Extract the inverse-transformed data\n",
    "    data = np.where(data < 0, 0, np.rint(data).astype(int)) # Set negatives to 0, round, and convert to int\n",
    "    data = torch.tensor(data.reshape(output_shape), dtype=torch.float) # Convert back to tensor\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c15b54-4b68-4b38-aed1-7d2f3974c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test data.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The DualGAT_Trans model to be evaluated.\n",
    "    - test_loader: DataLoader for the test data.\n",
    "    - criterion: Loss function used for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - mae: Mean Absolute Error of predictions.\n",
    "    - mape: Mean Absolute Percentage Error of predictions.\n",
    "    - rmse: Root Mean Squared Error of predictions.\n",
    "    - total_loss: Average loss over the test dataset.\n",
    "    - predictions: Tensor of model predictions.\n",
    "    - targets: Tensor of actual targets.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    predictions, targets = [], []\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient computation\n",
    "        for batch in zip(*test_loader):\n",
    "            out = model(batch) # Forward pass\n",
    "            loss = criterion(out, batch[0].y) # Compute loss\n",
    "            total_loss += loss.item()  # Accumulate loss\n",
    "            predictions.append(out) # Store predictions\n",
    "            targets.append(batch[0].y) # Store actual targets\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    # Inverse transform the predictions and targets\n",
    "    predictions = inv_transform(predictions, scaler, index_tf=2)\n",
    "    targets = inv_transform(targets, scaler, index_tf=2)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    mae = F.l1_loss(predictions, targets) # Mean Absolute Error\n",
    "    rmse = torch.sqrt(F.mse_loss(predictions, targets)) # Root Mean Squared Error\n",
    "    \n",
    "    return mae, rmse, total_loss / len(test_loader[0]), predictions, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdb873-e442-4b2f-9267-1a258e2380d8",
   "metadata": {},
   "source": [
    "# STEP 4. RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b71398f-7623-47ed-bccd-a5cf506b7849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, *train_data):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model on provided data.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The DualGAT_Trans model to be trained and evaluated.\n",
    "    - train_data: Tuple of datasets to be used for training and testing.\n",
    "\n",
    "    Returns:\n",
    "    - predictions: Final model predictions.\n",
    "    - targets: Actual targets corresponding to the predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader, test_loader = [], []\n",
    "\n",
    "    # Split data into training and testing sets, and create loaders\n",
    "    for data in train_data:\n",
    "        train_length = int(len(data) * Config.train_size) # Determine the split index\n",
    "        train, test = data[:train_length].copy(), data[train_length:].copy() # Split the data\n",
    "        train_loader.append(DataLoader(train, batch_size=Config.batch_size, shuffle=True))\n",
    "        test_loader.append(DataLoader(test, batch_size=Config.batch_size, shuffle=False))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(Config.epochs), desc=\"Training\"):\n",
    "        train_loss = train_model(model, train_loader, Config.optimizer, Config.criterion)\n",
    "        \n",
    "        if epoch % 5 == 0: # Evaluate every 5 epochs\n",
    "            mae, rmse, test_loss, predictions, targets = evaluate_model(model, test_loader, Config.criterion)\n",
    "            print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "            print(f'MAE: {mae:.4f}, RMSE: {rmse:.4f}')\n",
    "    \n",
    "    # Final evaluation after all epochs\n",
    "    mae, rmse, test_loss, predictions, targets = evaluate_model(model, test_loader, Config.criterion)\n",
    "    print(\"Final Test Results:\")\n",
    "    print(f'MAE: {mae:.4f}, RMSE: {rmse:.4f}, Loss: {test_loss:.4f}')\n",
    "    \n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82809033-65ad-4915-a681-e3fcf63a5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(model_designs, *inputs):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple models based on different designs.\n",
    "\n",
    "    Parameters:\n",
    "    - model_designs: A dictionary where keys are model types and values are the corresponding parameters.\n",
    "    - inputs: Tuple of datasets to be used for training and testing.\n",
    "\n",
    "    Returns:\n",
    "    - predictions_json: Dictionary containing predictions for each model type.\n",
    "    - targets_json: Dictionary containing targets for each model type.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop through each model type and run experiment\n",
    "    predictions_json, targets_json = {}, {}\n",
    "    for model_type, parameters in model_designs.items():\n",
    "        print(f\"Training {model_type} model\")\n",
    "        \n",
    "        # Create model and optimizer\n",
    "        model = DualGAT_Trans(model_params=parameters)\n",
    "\n",
    "        # Set optimizer and loss function\n",
    "        Config.optimizer = torch.optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
    "        Config.criterion = nn.MSELoss()\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        predictions, targets = run_model(model, *inputs)\n",
    "\n",
    "        # Store predictions and targets\n",
    "        predictions_json[model_type] = predictions\n",
    "        targets_json[model_type] = targets\n",
    "\n",
    "    return predictions_json, targets_json\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390476d6-73b5-48f9-b39d-86051acba645",
   "metadata": {},
   "source": [
    "# VISUALISE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce127a96-639c-4642-bbe4-284fde7cd4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(predictions, targets, title, x_label = 'Timestep'):\n",
    "    \"\"\"\n",
    "    Visualize predictions against true values over time.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: Tensor of model predictions.\n",
    "    - targets: Tensor of actual target values.\n",
    "    - title: Title of the plot.\n",
    "    - x_label: Label for the x-axis (default is 'Timestep').\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(14, 3))\n",
    "\n",
    "    # Average predictions and targets across the batch dimension\n",
    "    predictions = torch.mean(predictions, axis=1)\n",
    "    targets = torch.mean(targets, axis=1)\n",
    "\n",
    "    # Plot true values and predictions\n",
    "    plt.plot(targets, label='True Values', linewidth=1)\n",
    "    plt.plot(predictions, label='Predictions', linewidth=1)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Traffic Flow')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb519548-30ca-4fa0-a14e-68940a6f7290",
   "metadata": {},
   "source": [
    "# DECISION SUPPORT SYSTEM: REINFORCEMENT LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10459fca-9c76-4838-a044-b65410f9d3c5",
   "metadata": {},
   "source": [
    "## DATA CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d0681e6-5b71-4caf-97df-f9dbcf34b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rl_data(predictions, targets, meta_df, index_node_map):\n",
    "    \"\"\"\n",
    "    Create a DataFrame suitable for Reinforcement Learning (RL) from model predictions and targets.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: Tensor of model predictions.\n",
    "    - targets: Tensor of actual target values.\n",
    "    - meta_df: DataFrame containing metadata about the nodes.\n",
    "    - index_node_map: Mapping from index to node IDs.\n",
    "\n",
    "    Returns:\n",
    "    - rl_df: DataFrame with RL features and metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    rl_df = pd.DataFrame()\n",
    "\n",
    "    # Reshape predictions and targets for processing\n",
    "    targets_reshaped = targets.reshape(-1,len(index_node_map), targets.shape[1])\n",
    "    predictions_reshaped = predictions.reshape(-1,len(index_node_map), predictions.shape[1])\n",
    "\n",
    "    # Process each timestep: target and prediction data\n",
    "    for i in range(targets_reshaped.shape[0]):\n",
    "        \n",
    "        dummy_df = pd.DataFrame(targets_reshaped[i], columns = ['target_' + str(i) for i in range(targets.shape[1])])\n",
    "        dummy_df = dummy_df.reset_index()\n",
    "        dummy_dfp = pd.DataFrame(predictions_reshaped[i], columns = ['pred_' + str(i) for i in range(predictions.shape[1])])\n",
    "        dummy_dfp = dummy_dfp.reset_index()\n",
    "        dummy_df['freeway_id'] = dummy_df['index'].map(index_node_map)\n",
    "        dummy_df['timestep'] = i\n",
    "        dummy_df = pd.concat([dummy_df, dummy_dfp], axis=1)\n",
    "        rl_df = pd.concat([rl_df, dummy_df])\n",
    "\n",
    "    # Merge with metadata\n",
    "    rl_df = pd.merge(rl_df, meta_df, how='left', on = 'freeway_id')\n",
    "\n",
    "    # Calculate average values\n",
    "    rl_df['target_avg'] = np.mean(rl_df[['target_' + str(i) for i in range(targets.shape[1])]],axis=1)\n",
    "    rl_df['target_avg_per_lane'] = rl_df['target_avg']/rl_df['lanes']\n",
    "    rl_df['pred_avg'] = np.mean(rl_df[['pred_' + str(i) for i in range(predictions.shape[1])]],axis=1)\n",
    "    rl_df['pred_avg_per_lane'] = rl_df['pred_avg']/rl_df['lanes']\n",
    "\n",
    "    # Select relevant features\n",
    "    rl_features = ['timestep','freeway_id','highway','freeway_direction','lanes','target_avg_per_lane','pred_avg_per_lane']\n",
    "    rl_df = rl_df[rl_features]\n",
    "\n",
    "    return rl_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897c127-ade2-49aa-9a08-a1ee46345766",
   "metadata": {},
   "source": [
    "## OPPOSITE STATION MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ad294f-8c3f-41fd-9fcb-2870114cbbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_opposite_stations(df):\n",
    "    \"\"\"\n",
    "    Find the nearest opposite direction stations for each station.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing station metadata (latitude, longitude, freeway direction).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with nearest opposite stations and distances.\n",
    "    \"\"\"\n",
    "    # Group the dataframe by freeway_no\n",
    "    grouped = df.groupby('highway')\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # Function to add results to the list\n",
    "    def add_into_result(nearest_indices, dir1, dir2, distances):\n",
    "        for i, idx in enumerate(nearest_indices):\n",
    "            result.append({\n",
    "                    'station_id': dir1.iloc[i]['freeway_id'],#.name,\n",
    "                    'nearest_station_id': dir2.iloc[idx]['freeway_id'],#.name,\n",
    "                    'distance': distances[i, idx]\n",
    "                })\n",
    "        return 0\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        \n",
    "        # Split the group into two based on freeway_direction\n",
    "        dir1 = group[group['freeway_direction'] == group['freeway_direction'].iloc[0]]\n",
    "        dir2 = group[group['freeway_direction'] != group['freeway_direction'].iloc[0]]\n",
    "    \n",
    "        \n",
    "        if len(dir1) == 0 or len(dir2) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate distances between all pairs of stations in opposite directions\n",
    "        distances1 = cdist(dir1[['latitude', 'longitude']], dir2[['latitude', 'longitude']])\n",
    "        distances2 = cdist(dir2[['latitude', 'longitude']], dir1[['latitude', 'longitude']])\n",
    "\n",
    "        \n",
    "        # Find the index of the nearest station for each station\n",
    "        nearest_indices1 = np.argmin(distances1, axis=1)\n",
    "        nearest_indices2 = np.argmin(distances2, axis=1)\n",
    "        \n",
    "        add_into_result(nearest_indices1, dir1, dir2, distances1)\n",
    "        add_into_result(nearest_indices2, dir2, dir1, distances2)     \n",
    "    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe5697-4af0-4200-b935-ae7834676304",
   "metadata": {},
   "source": [
    "## MAP OPPOSITE STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37cf9d26-970e-43a0-837e-c4d09c2a431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_opp_stations(rl_df, opp_rl_df):\n",
    "    \"\"\"\n",
    "    Map the nearest opposite stations to the RL DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - rl_df: DataFrame with RL features and metadata.\n",
    "    - opp_rl_df: DataFrame with nearest opposite stations.\n",
    "\n",
    "    Returns:\n",
    "    - rl_df: Updated DataFrame with nearest opposite station features.\n",
    "    \"\"\"\n",
    "    \n",
    "    rl_df = pd.merge(rl_df, opp_rl_df[['station_id','nearest_station_id']], how = 'left', left_on = 'freeway_id', right_on = 'station_id')\n",
    "    \n",
    "    # Define features to include\n",
    "    rl_opp_features=['station_id','timestep','freeway_direction','lanes','target_avg_per_lane','pred_avg_per_lane']\n",
    "    rl_df = pd.merge(rl_df, rl_df[rl_opp_features], how = 'left', left_on = ['nearest_station_id','timestep'], right_on = ['station_id','timestep'], suffixes = (\"\",\"_nearest_station\")).drop(columns = ['freeway_id','nearest_station_id'])\n",
    "\n",
    "    return rl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ddecc-5da8-4f11-b461-b72fa16c5208",
   "metadata": {},
   "source": [
    "## GET STATE FROM TRAFFIC FLOW (REFERENCE & OPPOSITE STATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1a32a8e-b0ab-47f8-949c-0f30762c9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_index(state_space, input_values):\n",
    "    \"\"\"\n",
    "    Get the index of the state in the state space that matches the given input values.\n",
    "\n",
    "    Parameters:\n",
    "    - state_space: Array of all possible states.\n",
    "    - input_values: Values to find the nearest state index for.\n",
    "\n",
    "    Returns:\n",
    "    - index: Index of the matching state, or None if no match is found.\n",
    "    \"\"\"\n",
    "    \n",
    "    val1, val2 = input_values\n",
    "    \n",
    "    # Find the nearest lower or equal bin value for both elements\n",
    "    bin1 = np.max(state_space[:, 0][state_space[:, 0] <= val1])\n",
    "    bin2 = np.max(state_space[:, 1][state_space[:, 1] <= val2])\n",
    "    \n",
    "    # Find the index of the row in the state_space that matches the bin values\n",
    "    index = np.where((state_space[:, 0] == bin1) & (state_space[:, 1] == bin2))[0]\n",
    "    \n",
    "    if len(index) > 0:\n",
    "        return index[0]  # Return the first matching index\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510bbe1-1822-4961-9b1d-93c34a025040",
   "metadata": {},
   "source": [
    "## GET RELEVANT STATES TO UPDATE REWARDS BASED ON CURRENT STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aae70b6a-0123-45e7-9d01-7b32e9b2e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_state_indices(state_space, values, mode = 'reverse'):\n",
    "    \"\"\"\n",
    "    Get indices of related states based on given values and mode.\n",
    "\n",
    "    Parameters:\n",
    "    - state_space: Array of all possible states.\n",
    "    - values: Values to find related states for.\n",
    "    - mode: Mode for finding related states ('reverse' or 'non-reverse').\n",
    "\n",
    "    Returns:\n",
    "    - indices: Indices of related states.\n",
    "    \"\"\"\n",
    "    \n",
    "    val1, val2 = values\n",
    "    if mode == 'reverse':\n",
    "        # Condition 1: 1st element >= val1 and 2nd element = val2\n",
    "        cond1 = (state_space[:, 0] >= val1) & (state_space[:, 1] == val2)\n",
    "        # Condition 2: 1st element = val1 and 2nd element <= val2\n",
    "        cond2 = (state_space[:, 0] == val1) & (state_space[:, 1] <= val2)\n",
    "    else:\n",
    "        # Condition 1: 1st element >= val1 and 2nd element = val2\n",
    "        cond1 = (state_space[:, 0] <= val1) & (state_space[:, 1] == val2)\n",
    "        # Condition 2: 1st element = val1 and 2nd element <= val2\n",
    "        cond2 = (state_space[:, 0] == val1) & (state_space[:, 1] >= val2)\n",
    "    # Combine both conditions using logical OR\n",
    "    indices = np.where(cond1 | cond2)[0]\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b1ace-6e54-4676-9edd-6e94deaa8c0b",
   "metadata": {},
   "source": [
    "## LANE REVERSAL DECISION MODEL: REINFORCEMENT LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c1ced30-ad91-4101-9be4-ad8625db45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_model(rl_df, n_episodes=250):\n",
    "    \"\"\"\n",
    "    Train a Q-learning model for decision making based on RL data.\n",
    "\n",
    "    Parameters:\n",
    "    - rl_df: DataFrame with RL features.\n",
    "    - n_episodes: Number of episodes for training.\n",
    "\n",
    "    Returns:\n",
    "    - Q: Q-table after training.\n",
    "    - predict_action: Function to predict actions based on Q-table.\n",
    "    - state_space: Array of all possible states.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters\n",
    "    n_actions = 2  # [0: No Reversal, 1: Reversal]\n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 0.9\n",
    "    epsilon = 0.7  # Exploration rate\n",
    "    \n",
    "    max_value = rl_df['pred_avg_per_lane'].max()\n",
    "\n",
    "    # Create bins for state space\n",
    "    bin_size = 20\n",
    "    bins = np.arange(0, max_value + bin_size, bin_size)\n",
    "    \n",
    "    # Generate the cross product of the bins\n",
    "    x, y = np.meshgrid(bins, bins)\n",
    "    state_space = np.column_stack([x.ravel(), y.ravel()])\n",
    "\n",
    "    # Initialize Q-table\n",
    "    Q = np.zeros((len(state_space), n_actions))\n",
    "    \n",
    "    \n",
    "    # Reward function\n",
    "    def get_reward(pred_current, pred_opposite, actual_current, actual_opposite, action):\n",
    "        if action == 1:  # Reversal\n",
    "            if actual_current/max(1, actual_opposite) > 2 and actual_current>90: # Positive reward for successful decision\n",
    "                if pred_current/max(1, pred_opposite) > 2 and pred_current>90:\n",
    "                    return 20  \n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                if pred_current/max(1, pred_opposite) > 2 and pred_current>90: \n",
    "                    return 10  # Positive reward for semi-successful decision\n",
    "                else:\n",
    "                    return -10  # Negative reward for unsuccessful decision\n",
    "        else:  # No reversal\n",
    "            if not (actual_current/max(1, actual_opposite) > 2 and actual_current>90): # Positive reward for successful decision\n",
    "                if not (pred_current/max(1, pred_opposite) > 2 and pred_current>90):\n",
    "                    return 10  \n",
    "                else:\n",
    "                    return 5\n",
    "            else:  # Negative reward for unsuccessful decision\n",
    "                if not(pred_current/max(1, pred_opposite) > 2 and pred_current>90):\n",
    "                    return -5  \n",
    "                else:\n",
    "                    return -10 #\n",
    "    \n",
    "    # Q-learning algorithm\n",
    "    for episode in tqdm(range(n_episodes), leave=False):\n",
    "        for _, row in rl_df.iterrows():\n",
    "            row['pred_avg_per_lane'] = min(row['pred_avg_per_lane'], rl_df['target_avg_per_lane'].max())\n",
    "            row['pred_avg_per_lane_nearest_station'] = min(row['pred_avg_per_lane_nearest_station'], rl_df['target_avg_per_lane'].max())\n",
    "            state = get_state_index(state_space, (row['pred_avg_per_lane'],row['pred_avg_per_lane_nearest_station']))\n",
    "            action = np.random.randint(n_actions) if np.random.rand() < epsilon else np.argmax(Q[state, :])\n",
    "            \n",
    "            # Get reward based on actual values for verification\n",
    "            reward = get_reward(\n",
    "                row['pred_avg_per_lane'], \n",
    "                row['pred_avg_per_lane_nearest_station'], \n",
    "                row['target_avg_per_lane'], \n",
    "                row['target_avg_per_lane_nearest_station'], \n",
    "                action\n",
    "            )\n",
    "            \n",
    "            # Update Q-value\n",
    "            if action==1:\n",
    "                state_to_update = get_related_state_indices(state_space, state_space[state], mode = 'reverse')\n",
    "            else:\n",
    "                state_to_update = get_related_state_indices(state_space, state_space[state], mode = 'non-reverse')\n",
    "\n",
    "            # For simplicity, assume the next state is the same as the current state\n",
    "            next_state = state\n",
    "            best_next_action = np.argmax(Q[next_state, :])\n",
    "            Q[state_to_update, action] += learning_rate * (reward + discount_factor * Q[next_state, best_next_action] - Q[state_to_update, action])\n",
    "    \n",
    "    # Function to predict action based on Q-table\n",
    "    def predict_action(pred_current, pred_opposite):\n",
    "        state = get_state_index(state_space, (pred_current,pred_opposite))\n",
    "        return np.argmax(Q[state, :])\n",
    "\n",
    "    return Q, predict_action, state_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c327fb-279a-4c57-ba5f-d9287fc81ea5",
   "metadata": {},
   "source": [
    "## LANE REVERSAL MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dc62d64-17e3-409d-a2b2-143e75e3a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dss_result_plot(rl_df):\n",
    "    \"\"\"\n",
    "    Plot results of the decision support system (DSS) with lane reversal flag.\n",
    "\n",
    "    Parameters:\n",
    "    - rl_df: DataFrame with RL features including lane reversal.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot with different colors based on 'lane_reversal' flag\n",
    "    colors = {1: 'red', 0: 'green'}\n",
    "    plt.scatter(rl_df['pred_avg_per_lane_nearest_station'], rl_df['pred_avg_per_lane'],\n",
    "                c=rl_df['lane_reversal'].map(colors),\n",
    "                label=rl_df['lane_reversal'])\n",
    "    \n",
    "    # Adding labels and title\n",
    "    plt.ylabel('Reference Station Traffic')\n",
    "    plt.xlabel('Opp Station Traffic')\n",
    "    plt.title('Scatter Plot of Traffic with Lane Reversal Flag')\n",
    "    \n",
    "    # Creating a legend\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', label='Reversal',\n",
    "                          markerfacecolor='red', markersize=10),\n",
    "               plt.Line2D([0], [0], marker='o', color='w', label='No Reversal',\n",
    "                          markerfacecolor='green', markersize=10)]\n",
    "    plt.legend(handles=handles)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
